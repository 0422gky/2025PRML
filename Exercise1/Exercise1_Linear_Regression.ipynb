{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "In this exercise we are going to learn in practice how Linear Regression works.\n",
    "\n",
    "Before running the code, please refer to [this link](https://pytorch.org/get-started/locally/) to install PyTorch using the appropriate command for your system.\n",
    "\n",
    "You also need to install the `matplotlib` library using the following command:\n",
    "\n",
    "```bash\n",
    "pip install matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the device to be used for training. If a GPU is available, use it. Otherwise, use the CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Utils\n",
    "\n",
    "Plotting is quite important in machine learning which helps us quickly get intuition from enormous data points and gain intuition. Plus they also do good to your illustration. Matplotlib is one of the most commonly used plotting libraries in python. \n",
    "\n",
    "Seaborn, Nineplot and Plotly are also powerful and it's important to master at least one of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Toy Data\n",
    "\n",
    "We have prepared the toy data for you. But do not just skip this part! Skim through it and think:\n",
    "\n",
    "- Why are we declaring a known function and add random noise to samples from it? What are the practical counterparts of them?\n",
    "\n",
    "- Why do we choose Gaussian distribution as the noise instead of the other distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return 2 * x + 1\n",
    "\n",
    "def create_toy_data(func, interval, sample_num, noise = 0.0, add_outlier = False, outlier_ratio = 0.001):\n",
    "    \"\"\"\n",
    "    generate data with the given function\n",
    "    \n",
    "    input:\n",
    "       - func: the input function\n",
    "       - interval: the range of values of x, a tuple (start, end)\n",
    "       - sample_num: number of samples\n",
    "       - noise: the standard deviation of Gaussian noise\n",
    "       - add_outlier: whether to generate outliers\n",
    "       - outlier_ratio: proportion of outliers\n",
    "       \n",
    "    output:\n",
    "       - X: samples, shape = [n_samples,1]\n",
    "       - y: labels, shape = [n_samples,1]\n",
    "    \"\"\"\n",
    "    \n",
    "    X = torch.rand(sample_num,1, device=device) * (interval[1]-interval[0]) + interval[0]\n",
    "    y = func(X)\n",
    "\n",
    "    # add Gaussian noise\n",
    "    epsilon = torch.normal(0, noise, (sample_num,1), device=device)\n",
    "    y = y + epsilon\n",
    "    \n",
    "    # add outlier\n",
    "    if add_outlier:\n",
    "        outlier_num = int(sample_num * outlier_ratio)\n",
    "        if outlier_num != 0:\n",
    "            outlier_idx = torch.randint(sample_num, size = [outlier_num,1], device=device)\n",
    "            y[outlier_idx] = y[outlier_idx] * 5\n",
    "            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black line in the following figure stands for the underlying data distribution, circles for trainig data and triangles for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = (0,1)\n",
    "train_num = 15\n",
    "test_num = 10\n",
    "noise = 0.2\n",
    "X_train, y_train = create_toy_data(func=func, interval=interval, sample_num=train_num, noise=noise)\n",
    "X_test, y_test = create_toy_data(func=func, interval=interval, sample_num=test_num, noise=noise)\n",
    "\n",
    "X_underlying = torch.linspace(interval[0],interval[1],100, device=device)\n",
    "y_underlying = func(X_underlying)\n",
    "\n",
    "# plot\n",
    "\"\"\"_summary_ draw the plot of the underlying function and the training and test data\"\"\"\n",
    "rc('font', family='times new roman')\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0)\n",
    "plt.plot(X_underlying.cpu(), y_underlying.cpu(), c='#000000', label=r\"Ground Truth: $y = 2x + 1$\")\n",
    "plt.scatter(X_train.cpu(), y_train.cpu(), facecolor=\"none\", edgecolor='#e4007f', s=50, label=\"Train Data\")\n",
    "plt.scatter(X_test.cpu(), y_test.cpu(), facecolor=\"none\", edgecolor=\"r\", marker = '^', s=50, label=\"Test Data\")\n",
    "plt.legend(fontsize='x-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch for Linear Regression\n",
    "\n",
    "PyTorch is a powerful deep learning framework which is widely used in academia and industry. It is also a good tool for learning machine learning. In this exercise, we are going to use PyTorch to implement a simple linear regression model.\n",
    "\n",
    "We are going to use two methods to implement the linear regression model. The first one is to use the closed-form solution, and the second one is to use the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-form Solution without Bias\n",
    "\n",
    "Here we present an example of using the closed-form solution to solve a $y = ax$-style linear regression problem without bias. The closed-form solution is as follows:\n",
    "\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^TY\n",
    "$$\n",
    "\n",
    "where $\\theta$ is the parameter vector, $X$ is the feature matrix, and $Y$ is the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate closed-form solution using the normal equation\n",
    "def normal_equation(X, y):\n",
    "    \"\"\"\n",
    "    Calculate the closed-form solution of $y = ax$ using the normal equation\n",
    "    \"\"\"\n",
    "    \n",
    "    theta = torch.inverse(X.T @ X) @ X.T @ y\n",
    "    \n",
    "    return theta.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the solution using the normal equation\n",
    "theta_closed_form = normal_equation(X_train, y_train)\n",
    "theta_closed_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the regression line\n",
    "y_pred = X_underlying * theta_closed_form\n",
    "plt.plot(X_underlying.cpu(), y_underlying.cpu(), c='#000000', label=r\"Ground Truth: $y = 2x + 1$\")\n",
    "plt.plot(X_underlying.cpu(), y_pred.cpu(), c='#0075ad', label=f\"Regression Line: $y = {theta_closed_form:.2f}x$\")\n",
    "plt.scatter(X_train.cpu(), y_train.cpu(), facecolor=\"none\", edgecolor='#e4007f', s=50, label=\"Train Data\")\n",
    "plt.scatter(X_test.cpu(), y_test.cpu(), facecolor=\"none\", edgecolor=\"r\", marker = '^', s=50, label=\"Test Data\")\n",
    "plt.legend(fontsize='x-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm that is used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. The formula is as follows:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\nabla_{\\theta}\\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "where $\\theta$ is the parameter vector, $\\alpha$ is the learning rate, and $\\mathcal{L}(\\theta)$ is the loss function.\n",
    "\n",
    "Here we present an example of using the gradient descent algorithm to solve a $y = ax$-style linear regression problem without bias. The loss function is the mean squared error (MSE) function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\theta x_i)^2\n",
    "$$\n",
    "\n",
    "where $n$ is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Gradient Descent algorithm\n",
    "def gradient_descent(X, y, learning_rate, n_epochs):\n",
    "    \"\"\"\n",
    "    Implement the Gradient Descent algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    theta = torch.zeros(n_features,1, device=device)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # calculate the gradient\n",
    "        gradient = 2/n_samples * X.T @ (X @ theta - y)\n",
    "        \n",
    "        # update the parameter\n",
    "        theta = theta - learning_rate * gradient\n",
    "        \n",
    "    return theta.item()\n",
    "\n",
    "# Find the solution using the Gradient Descent algorithm\n",
    "learning_rate = 0.1\n",
    "n_epochs = 100\n",
    "theta_gradient_descent = gradient_descent(X_train, y_train, learning_rate, n_epochs)\n",
    "theta_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the regression line\n",
    "y_pred = X_underlying * theta_gradient_descent\n",
    "plt.plot(X_underlying.cpu(), y_underlying.cpu(), c='#000000', label=r\"Ground Truth: $y = 2x + 1$\")\n",
    "plt.plot(X_underlying.cpu(), y_pred.cpu(), c='#00c92a', label=f\"Closed Form: $y = {theta_closed_form:.2f}x$\")\n",
    "plt.plot(X_underlying.cpu(), y_pred.cpu(), c='#0075ad', label=f\"Gradient Descent: $y = {theta_gradient_descent:.2f}x$\")\n",
    "plt.scatter(X_train.cpu(), y_train.cpu(), facecolor=\"none\", edgecolor='#e4007f', s=50, label=\"Train Data\")\n",
    "plt.scatter(X_test.cpu(), y_test.cpu(), facecolor=\"none\", edgecolor=\"r\", marker = '^', s=50, label=\"Test Data\")\n",
    "plt.legend(fontsize='x-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What metric does evaluate_rmse compute? How is it different from the standard deviation (std)? Explain in one sentence.\n",
    "\n",
    "2. Read the parameter block at the top of data_generate.py. When D=3, W=[1.0, 0.5, -0.8], B=1, and NOISE_STD_TEST=2.0:\n",
    "a) How many columns are in the generated train.txt and test_X.txt, respectively? What does the last column represent?\n",
    "b) What is the approximate value of ||w||_2 for this setting?\n",
    "\n",
    "3. When changing the model from “no bias” to “with bias,” what geometric change occurs to the regression line, and why does this usually reduce error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers\n",
    "Please answer the questions here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Your task is to modify the provided code and implement both a closed-form solution and a gradient descent algorithm to solve the linear regression problem $y = ax + b$ with bias and polynomial linear regression. Concretely, you are required to:\n",
    "\n",
    "1. Calculate the closed-form solution for the linear regression problem with bias. Implement the function `closed_form_solution_with_bias` to calculate the weight $\\theta$ and bias $b$.\n",
    "\n",
    "2. Calculate the gradient of the MSE loss function with respect to the weight and bias. Implement the function `gradient_descent_with_bias` to calculate the gradients and iteratively update the weight and bias using the gradient descent algorithm.\n",
    "\n",
    "3. Extend the implementation to support multivariate linear regression when features are multi-dimensional.\n",
    "\n",
    "4. Evaluate the performance of both the closed-form solution and the gradient descent algorithm on the test dataset. Implement the function evaluate to compute the RMSE of the predictions on the test data. Explain the meaning of the Root Mean Squared Error (RMSE) metric."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
