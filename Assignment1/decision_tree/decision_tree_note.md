# decision tree
@import "decision_tree_split.png"
结点不纯度的衡量： 
1. entropy
@import "entropy.png"
@import "info_gain.png"
C4.5算法；惩罚子节点数
@import "c4.5alg.png"
2. Gini系数 CART算法
@import "gini_index.png"
3. 误分类率
@import "misclassify.png"

## pre_prun
@import "prun.png"




在 **决策树 (Decision Tree)** 里，**Information Gain (信息增益)** 是用来衡量一个特征对分类结果的“贡献度”的指标。构建决策树时，我们希望每次划分都能让数据“更纯”（即子节点里的样本尽量属于同一类），而信息增益正是用来量化这种“纯度提升”的。

---

### 1. 熵（Entropy）

首先用 **熵** 来表示样本集合的不确定性：

[
H(S) = - \sum_{k=1}^K p_k \log_2(p_k)
]

其中：

* (S) 是一个样本集合，
* (K) 是类别数，
* (p_k) 是集合 (S) 中第 (k) 类的比例。

👉 熵越大，表示越混乱；熵越小，表示越纯。
例如：

* 如果集合里全是同一类，熵 = 0；
* 如果两类各占一半，熵 = 1（最混乱）。

---

### 2. 信息增益（Information Gain）

假设我们用特征 (A) 来划分集合 (S)，会把数据分成若干子集 ({S_v})。此时：

[
IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)
]

含义：

* (H(S))：划分前的熵（原始混乱度）；
* (\sum \frac{|S_v|}{|S|} H(S_v))：划分后的加权平均熵；
* (IG(S, A))：熵的减少量，也就是 **用特征 A 划分后信息提升了多少**。

---

### 3. 举个例子

假设现在有 10 个样本：

* 6 个正类 (Yes)
* 4 个负类 (No)

熵为：
[
H(S) = - \frac{6}{10}\log_2 \frac{6}{10} - \frac{4}{10}\log_2 \frac{4}{10} \approx 0.97
]

假设用一个特征 A 划分，得到：

* 子集 1（6 个样本，全是 Yes）：熵 = 0
* 子集 2（4 个样本，全是 No）：熵 = 0

划分后的加权平均熵 = 0，
所以信息增益：

[
IG(S, A) = 0.97 - 0 = 0.97
]

说明这个特征非常好，可以直接把数据完全分开。

---

### 4. 总结

* **熵**：度量集合的不确定性。
* **信息增益**：划分前后不确定性的减少量。
* 决策树（如 ID3 算法）每次选择 **信息增益最大的特征** 来划分节点。

---

很好！你已经理解了 **信息增益 (Information Gain, IG)**，那 **信息增益率 (Information Gain Ratio, IGR)** 就是对它的一种改进。

---

## 1. 问题背景

* **信息增益 (IG)** 有一个缺点：
  它偏好 **取值很多的特征**。
  比如一个“身份证号”特征，每个样本几乎都有唯一取值，用它来划分时，每个样本都落到自己的叶子节点里，这样熵确实减小很多，IG 非常大，但模型完全过拟合，泛化很差。

---

## 2. 信息增益率 (Information Gain Ratio)

为了修正这种偏好，**C4.5 决策树**提出了 **信息增益率**：

[
IGR(S, A) = \frac{IG(S, A)}{IV(A)}
]

其中：

* (IG(S, A))：信息增益；
* (IV(A))：特征 (A) 的 **固有值 (Intrinsic Value)**，计算公式为：

[
IV(A) = - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} \log_2 \frac{|S_v|}{|S|}
]

👉 (IV(A)) 实际上就是“按特征 A 划分时，划分结果本身的不确定性”。
如果某个特征有很多不同取值，那它的 (IV(A)) 会很大，从而把 IG “压缩”下去。

---

## 3. 举个例子

* 假设有个特征 **性别**（取值少：男、女），它划分后得到比较好的 IG。
* 假设另一个特征 **身份证号**（取值多，每人唯一），IG 会非常大，但因为取值太多，(IV(A)) 也非常大，结果 IGR 反而比较小。

这样，IGR 就避免了偏向“取值多”的特征。

---

## 4. 总结

* **信息增益 (IG)**：衡量划分后熵的减少量。
* **信息增益率 (IGR)**：在 IG 的基础上，加上一个归一化惩罚，避免偏好取值多的特征。
* **决策树算法区别**：

  * ID3 用 **信息增益**。
  * C4.5 用 **信息增益率**。
  * CART 用 **基尼指数 (Gini Index)**。

---

